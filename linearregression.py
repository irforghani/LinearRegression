# -*- coding: utf-8 -*-
"""LinearRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kjV6P2XF6pZXV5M0Yam0Caj85xgv3Gmz

![alt text](http://icee2017.kntu.ac.ir:81/files_site/sponsorfn_pic/r_83_161103171713.png)

# Linear Regression with SkLearn

## Ali Forghani ElahAbadi
## Kharazmi University, Tehran, IRAN

### Python Language (version 3.6)
"""

# -*- coding: utf-8 -*-
"""
Created on Sat Oct 27 03:14:50 2018

@author: Ali Forghani
"""

#library import
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics
'''
x_axis = np.arange(-10, 10, 0.001)
# Mean = 0, SD = 2.
plt.plot(x_axis, norm.pdf(x_axis,0,2.3))
plt.show()

'''

from random import gauss
import random

#create gaussian noise
noise = np.random.normal(0, 2.3, 200)

plt.style.use('seaborn') # pretty matplotlib plots
plt.rcParams['figure.figsize'] = (15, 7)


#generate y by equation
x = []
y = []
y_orig = []
for i in range(0, 200):
    t1 = random.uniform(0.0, 2.0)
    x.append(t1)
    y.append(pow(t1, 3) - (3 * pow(t1, 2)) - t1 + 3 + noise[random.randint(0, 199)])
    y_orig.append(pow(t1, 3) - (3 * pow(t1, 2)) - t1 + 3)
plt.scatter(x, y_orig, c = 'black', s =1)

x = np.asarray(x)
y = np.asarray(y)

#plot raw data
plt.scatter(x, y)
plt.legend(('raw data without noise', 'raw data with noise'),
           loc='upper center', shadow=True, frameon=True)
plt.show()

x = x.reshape(200, 1)
y = y.reshape(200, 1)

print('-----------------------'+str(x.shape))

#print train and test data size
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.1 , random_state = 0)
print ("*************Trainnin data****************")
print ("X train shape : " ,X_train.shape)
print ("y train shape : " ,y_train.shape)

print ("*************Testing data******************")
print ("X test shape : " ,X_test.shape)
print ("y test shape : " ,y_test.shape)

# Fitting Simple Linear Regression to the Training set
regressor = LinearRegression()
regressor.fit(X_train, y_train)
print("Function for the first Graph")
m = regressor.coef_[0]
b = regressor.intercept_
print(' y = {0} * x + {1}'.format(m, b))

# Predicting the Test set results
y_pred = regressor.predict(X_test)
print (y_pred)
plt.scatter(x, y_orig, c = 'black', s =1)
 #Visualising the Training set results
plt.scatter(X_train, y_train, color = 'blue', s = 10)
plt.plot(X_train, regressor.predict(X_train), color = 'red')
plt.title('S')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend(('Regression Line', 'raw data without noise', 'raw data with noise'),
           loc='upper center', shadow=True, frameon=True)
plt.show()

# Visualising the Test set results

plt.scatter(X_test, y_test, color = 'blue')
plt.plot(X_train, regressor.predict(X_train), color = 'red')
plt.title('S')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend(('Regression Line', 'Test data'),
           loc='upper center', shadow=True, frameon=True)
plt.show()

#calculate errors
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))